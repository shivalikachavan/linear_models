---
title: "cross_validation"
author: "Shivalika Chavan"
date: "2025-11-11"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(modelr)
library(mgcv)
library(p8105.datasets)

set.seed(1)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


```{r}
data("lidar")

lidar_df = 
  lidar |> 
  mutate(id = row_number())

lidar_df |> 
  ggplot(aes(x = range, y = logratio)) + 
  geom_point()
```


```{r}
train_df = sample_frac(lidar_df, size = .8) |> 
  arrange(id)
test_df = anti_join(lidar_df, train_df, by = "id")
```

Look at the train and test df
```{r}
ggplot(train_df, aes(x = range, y = logratio)) + 
  geom_point() + 
  geom_point(data = test_df, color = "red")
```

Fit a few models to the training data `train_df`
```{r}
linear_model = lm(logratio ~ range, data = train_df)
smooth_model = gam(logratio ~ s(range), data = train_df) #Generalized additive model (anything that's non-linear), range is smoothed
wiggly_model = gam(logratio ~ s(range, k = 30), sp = 10e-6, , data = train_df) # using 30 polynomials, sp smoothing parameter by not a lot
```

Looking at `linear_model`
```{r}
train_df |> 
  add_predictions(linear_model) |> 
  ggplot(aes(x = range, y = logratio)) +
  geom_point() +
  geom_line(aes(y = pred), color = "red")
```

Looking at `smooth_model`
```{r}
train_df |> 
  add_predictions(smooth_model) |> 
  ggplot(aes(x = range, y = logratio)) +
  geom_point() +
  geom_line(aes(y = pred), color = "red")
```

Looking at `wiggly_model`
```{r}
train_df |> 
  add_predictions(wiggly_model) |> 
  ggplot(aes(x = range, y = logratio)) +
  geom_point() +
  geom_line(aes(y = pred), color = "red")
```

Computing RMSE
```{r}
rmse(linear_model, data = test_df)
rmse(smooth_model, data = test_df)
rmse(wiggly_model, data = test_df)
```

Automating 100 iterations of CV train/test split. Defaults to 80/20 split
```{r}
cv_df = crossv_mc(lidar_df, n = 100) |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )
```

```{r}
cv_df |> pull(train) |> nth(3)
```

Fitting models again and again
```{r}
cv_df = 
  cv_df |> 
  mutate(
    # Using anonymous function
    linear_fits = map(train, \(df) lm(logratio ~ range, data = df)),
    smooth_fits = map(train, \(df) gam(logratio ~ s(range), data = df)),
    wiggly_fits = map(train, \(df) gam(logratio ~ s(range, k = 30), sp = 10e-6, , data = df))
  ) |> 
  mutate(
    rmse_linear_fits = map2_dbl(linear_fits, test, rmse),
    rmse_smooth_fits = map2_dbl(smooth_fits, test, rmse),
    rmse_wiggly_fits = map2_dbl(wiggly_fits, test, rmse)
  )
```

Looking at distribution of RMSE
```{r}
cv_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin()
```

